{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:41, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.675916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.699100</td>\n",
       "      <td>0.676274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.655500</td>\n",
       "      <td>0.714644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.553800</td>\n",
       "      <td>0.658355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>0.666103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>0.668720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.183600</td>\n",
       "      <td>0.793753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.080800</td>\n",
       "      <td>0.977790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>1.128682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>1.268721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>1.386277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>1.496838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>1.573819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>1.629568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.677895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>1.719961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>1.756284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>1.784926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.809986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.828776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.846859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>1.865724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.883288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.898967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.913249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>1.925761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>1.936402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.948851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.958934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>1.969590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>1.979545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.988525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.996613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>2.003769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>2.010807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>2.017071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>2.022756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>2.028523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>2.033349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>2.037670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>2.041902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>2.045829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>2.049243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>2.052024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>2.054301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>2.056474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>2.058077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>2.059197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>2.059887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>2.060129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model Accuracy: 0.67\n",
      "'Scientists invent teleportation technology, changing travel forever!' â†’ Fake News\n",
      "'World Health Organization announces breakthrough in malaria vaccine.' â†’ Real News\n",
      "'Shocking discovery: Atlantis city found under the ocean!' â†’ Fake News\n",
      "'New smartphone released with groundbreaking AI features.' â†’ Real News\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries (if not installed)\n",
    "# !pip install transformers tensorflow scikit-learn pandas numpy torch\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Create a Fake News dataset\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"Breaking: Scientists discover a cure for COVID-19!\",  # Fake\n",
    "        \"NASA confirms water on the moon, groundbreaking discovery!\",  # Real\n",
    "        \"Experts warn of stock market collapse due to secret government policies.\",  # Fake\n",
    "        \"Government announces new tax relief for small businesses.\",  # Real\n",
    "        \"Shocking: Aliens found living in Area 51!\",  # Fake\n",
    "        \"New study finds link between exercise and improved mental health.\",  # Real\n",
    "        \"Politician caught hiding millions in offshore accounts.\",  # Fake\n",
    "        \"Medical researchers develop breakthrough cancer treatment.\",  # Real\n",
    "        \"Secret messages found in ancient pyramids predict end of the world!\",  # Fake\n",
    "        \"Tech company launches revolutionary AI that changes programming forever.\",  # Real,\n",
    "    ],\n",
    "    \"label\": [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 = Fake, 0 = Real\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2. Tokenization using BERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Custom dataset class for PyTorch\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=20):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"label\"], test_size=0.3, random_state=42)\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = FakeNewsDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n",
    "test_dataset = FakeNewsDataset(X_test.tolist(), y_test.tolist(), tokenizer)\n",
    "\n",
    "# 3. Load Pre-trained DistilBERT Model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2).to(device)\n",
    "\n",
    "# 4. Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "# 5. Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# 6. Train the Model\n",
    "trainer.train()\n",
    "\n",
    "# 7. Evaluate the Model\n",
    "def compute_accuracy(model, dataset):\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(dataset, batch_size=2)\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return accuracy_score(true_labels, predictions)\n",
    "\n",
    "bert_accuracy = compute_accuracy(model, test_dataset)\n",
    "print(f\"BERT Model Accuracy: {bert_accuracy:.2f}\")\n",
    "\n",
    "# 8. Test with new headlines\n",
    "new_headlines = [\n",
    "    \"Scientists invent teleportation technology, changing travel forever!\",\n",
    "    \"World Health Organization announces breakthrough in malaria vaccine.\",\n",
    "    \"Shocking discovery: Atlantis city found under the ocean!\",\n",
    "    \"New smartphone released with groundbreaking AI features.\"\n",
    "]\n",
    "\n",
    "# Tokenize and Predict\n",
    "new_encodings = tokenizer(new_headlines, padding=True, truncation=True, max_length=20, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**new_encodings)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "# Print Predictions\n",
    "for headline, pred in zip(new_headlines, predictions):\n",
    "    category = \"Fake News\" if pred == 1 else \"Real News\"\n",
    "    print(f\"'{headline}' â†’ {category}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why use BERT model? Where does it make sense to use it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
